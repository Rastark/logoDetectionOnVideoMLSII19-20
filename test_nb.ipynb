{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T11:31:20.683138Z",
     "start_time": "2020-12-18T11:31:20.428688Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 18 12:31:20 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.48                 Driver Version: 410.48                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-SXM2...  Off  | 00000000:05:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    41W / 300W |   8035MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-SXM2...  Off  | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    33W / 300W |     10MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-SXM2...  Off  | 00000000:84:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    34W / 300W |     10MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-SXM2...  Off  | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    42W / 300W |   7462MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     30814      C   ...nvidia/anaconda3/envs/tf-gpu/bin/python  2761MiB |\n",
      "|    0     48618      C   ...nvidia/anaconda3/envs/tf-gpu/bin/python  1921MiB |\n",
      "|    0     48784      C   ...nvidia/anaconda3/envs/tf-gpu/bin/python  3343MiB |\n",
      "|    3     26354      C   ...nvidia/anaconda3/envs/tf-gpu/bin/python  1671MiB |\n",
      "|    3     64350      C   ...nvidia/anaconda3/envs/tf-gpu/bin/python  5781MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T18:18:09.135187Z",
     "start_time": "2020-12-17T18:18:03.168648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nvgpu\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/1f/1543808105e377bb154ff61c2623cdf76a330b983b22c285a14d03e7431b/nvgpu-0.9.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /home/nvidia/.local/lib/python3.7/site-packages (from nvgpu) (1.15.0)\n",
      "Collecting arrow (from nvgpu)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/bc/ebc1afb3c54377e128a01024c006f983d03ee124bc52392b78ba98c421b8/arrow-0.17.0-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 4.1MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: flask in /home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages (from nvgpu) (1.0.2)\n",
      "Collecting pynvml; python_version >= \"3.6\" (from nvgpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/1b/1a/a25c143e1d2f873d67edf534b269d028dd3c20be69737cca56bf28911d02/pynvml-8.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: termcolor in /home/nvidia/.local/lib/python3.7/site-packages (from nvgpu) (1.1.0)\n",
      "Collecting flask-restful (from nvgpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/83/d0d33c971de2d38e54b0037136c8b8d20b9c83d308bc6c220a25162755fd/Flask_RESTful-0.3.8-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests in /home/nvidia/.local/lib/python3.7/site-packages (from nvgpu) (2.23.0)\n",
      "Requirement already satisfied: psutil in /home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages (from nvgpu) (5.6.3)\n",
      "Requirement already satisfied: pandas in /home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages (from nvgpu) (0.24.2)\n",
      "Collecting ansi2html (from nvgpu)\n",
      "  Downloading https://files.pythonhosted.org/packages/c6/85/3a46be84afbb16b392a138cd396117f438c7b2e91d8dc327621d1ae1b5dc/ansi2html-1.6.0-py3-none-any.whl\n",
      "Collecting tabulate (from nvgpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/c4/f4/770ae9385990f5a19a91431163d262182d3203662ea2b5739d0fcfc080f1/tabulate-0.8.7-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /home/nvidia/.local/lib/python3.7/site-packages (from arrow->nvgpu) (2.8.1)\n",
      "Requirement already satisfied: click>=5.1 in /home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages (from flask->nvgpu) (7.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages (from flask->nvgpu) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10 in /home/nvidia/.local/lib/python3.7/site-packages (from flask->nvgpu) (2.11.2)\n",
      "Requirement already satisfied: Werkzeug>=0.14 in /home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages (from flask->nvgpu) (0.14.1)\n",
      "Requirement already satisfied: pytz in /home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages (from flask-restful->nvgpu) (2018.9)\n",
      "Collecting aniso8601>=0.82 (from flask-restful->nvgpu)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/4e/760c0aaf32034e2da98e1ac6d83b6ffc6d1301132af54c3950ee07785bfa/aniso8601-8.1.0-py2.py3-none-any.whl (44kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 13.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/nvidia/.local/lib/python3.7/site-packages (from requests->nvgpu) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->nvgpu) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->nvgpu) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages (from requests->nvgpu) (1.24.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages (from pandas->nvgpu) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/nvidia/.local/lib/python3.7/site-packages (from Jinja2>=2.10->flask->nvgpu) (1.1.1)\n",
      "Installing collected packages: arrow, pynvml, aniso8601, flask-restful, ansi2html, tabulate, nvgpu\n",
      "Successfully installed aniso8601-8.1.0 ansi2html-1.6.0 arrow-0.17.0 flask-restful-0.3.8 nvgpu-0.9.0 pynvml-8.0.4 tabulate-0.8.7\n"
     ]
    }
   ],
   "source": [
    "!pip install nvgpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T18:18:20.373215Z",
     "start_time": "2020-12-17T18:18:20.184579Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c5df034ef045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnvgpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mavail_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnvgpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavail_gpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/nvgpu/__init__.py\u001b[0m in \u001b[0;36mavailable_gpus\u001b[0;34m(max_used_percent)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mavailable_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_used_percent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgpu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgpu_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mem_used_percent'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_used_percent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/nvgpu/__init__.py\u001b[0m in \u001b[0;36mgpu_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_cmd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nvidia-smi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcuda_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CUDA Version: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcuda_version\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mline_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import nvgpu\n",
    "\n",
    "avail_gpus = nvgpu.available_gpus()\n",
    "print(avail_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T11:35:47.186832Z",
     "start_time": "2020-12-18T11:35:47.182324Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T11:35:48.009173Z",
     "start_time": "2020-12-18T11:35:48.005210Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T11:35:49.377076Z",
     "start_time": "2020-12-18T11:35:48.782295Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Parts of the U-Net model \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Downscaler(nn.Module):\n",
    "    \"\"\"Double conv 3x3, then max pool 2x2 stride 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Upscaler(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = DoubleConv(in_channels, in_channels)\n",
    "        self.upscale = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.double_conv(x)\n",
    "        return self.upscale(x)        \n",
    "\n",
    "\n",
    "class OutSoftmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OutSoftmax, self).__init__()\n",
    "        self.softmax = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(x)\n",
    "\n",
    "\n",
    "# Conditioning Branch\n",
    "class OneOneConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OneOneConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "            # nn.Conv2d(out_channels, out_channels, kernel_size=1), # kernel_size=3, padding=1\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# class ConditioningConcat(nn.Module):\n",
    "#     def __init__(self, tile_concat):\n",
    "#         super(ConditioningConcat, self).__init__()\n",
    "#         self.filtered_tile = OneOneConv(self.tile_concat)       # Here or in forward method?\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return torch.cat(x, self.filtered_tile, dim=-1)\n",
    "\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, batch_norm=False, cfg='A', in_channels=3):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        self.cfgs = {\n",
    "            'A': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M', 512, 512, 512, 'M'],     # 6 \"evenly\" distributed maxpools to reduce dims to 1x1x512 \n",
    "            'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M', 'M'],     # 6 maxpools to reduce dims to 1x1x512\n",
    "        }\n",
    "        self.cfg = self.cfgs[cfg]\n",
    "\n",
    "        self.layers = []\n",
    "        for v in self.cfg:\n",
    "            if v == 'M':\n",
    "                self.layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                if self.batch_norm:\n",
    "                    self.layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    self.layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        self.vgg16 = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vgg16(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T11:35:49.746955Z",
     "start_time": "2020-12-18T11:35:49.742598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T11:35:50.692848Z",
     "start_time": "2020-12-18T11:35:50.665043Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogoDetection(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_channels: int = 3,\n",
    "                 batch_norm=False,\n",
    "                 vgg_cfg: str = 'A'):\n",
    "        super(LogoDetection, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "\n",
    "        # Encoder steps\n",
    "        self.input_layer = Downscaler(self.n_channels, 64)\n",
    "        # self.input_layer.wei\n",
    "        self.down_layer1 = Downscaler(64, 128)\n",
    "        self.down_layer2 = Downscaler(128, 256)\n",
    "        self.down_layer3 = Downscaler(256, 512)\n",
    "        self.down_layer4 = Downscaler(512, 512)  # 1/(2^5)*(width x height) x 512\n",
    "\n",
    "        # Conditioning Module\n",
    "        self.latent_repr = VGG16(batch_norm, vgg_cfg)\n",
    "        self.one_conv1 = OneOneConv(576, 64)  # 64+512\n",
    "        self.one_conv2 = OneOneConv(640, 128)  # 128+512\n",
    "        self.one_conv3 = OneOneConv(768, 256)  # 256+512\n",
    "        self.one_conv4 = OneOneConv(1024, 512)  # 512+512\n",
    "\n",
    "        # Decoder steps\n",
    "        self.up1 = Upscaler(1536, 512)  # 1024+512\n",
    "        self.up2 = Upscaler(1024, 256)  # 512*2\n",
    "        self.up3 = Upscaler(512, 128)  # 256*2\n",
    "        self.up4 = Upscaler(256, 64)  # 128*2\n",
    "        self.up5 = Upscaler(128, 1)  # 64*2\n",
    "        self.output_layer = OutSoftmax()\n",
    "\n",
    "        # with torch.no_grad():\n",
    "        #     self.input_layer.weight = torch.nn.Parameter()\n",
    "\n",
    "    def forward(self, query, target):\n",
    "        # query = samples[:, 0]\n",
    "        # target = samples[:, 1]\n",
    "        z = self.latent_repr(query)\n",
    "        # print(z.shape)\n",
    "\n",
    "        # Encoder + Conditioning\n",
    "        x = self.input_layer(target)\n",
    "\n",
    "        tile = z.expand(z.shape[0], z.shape[1], 128, 128)\n",
    "        # print(tile.shape)\n",
    "        x1 = torch.cat((x, tile), dim=1)\n",
    "        x = self.down_layer1(x)\n",
    "\n",
    "        tile = z.expand(z.shape[0], z.shape[1], 64, 64)\n",
    "        x2 = torch.cat((x, tile), dim=1)\n",
    "        x = self.down_layer2(x)\n",
    "\n",
    "        tile = z.expand(z.shape[0], z.shape[1], 32, 32)\n",
    "        x3 = torch.cat((x, tile), dim=1)\n",
    "        x = self.down_layer3(x)\n",
    "\n",
    "        tile = z.expand(z.shape[0], z.shape[1], 16, 16)\n",
    "        x4 = torch.cat((x, tile), dim=1)\n",
    "        x = self.down_layer4(x)\n",
    "\n",
    "        tile = z.expand(z.shape[0], z.shape[1], 8, 8)\n",
    "        x5 = torch.cat((x, tile), dim=1)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # Decoder + Conditioning\n",
    "        x = torch.cat((x, x5), dim=1)\n",
    "        # print(x.shape)\n",
    "        x = self.up1(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x4 = self.one_conv4(x4)\n",
    "        x = torch.cat((x, x4), dim=1)\n",
    "        # del x4\n",
    "        x = self.up2(x)\n",
    "\n",
    "        x3 = self.one_conv3(x3)\n",
    "        x = torch.cat((x, x3), dim=1)\n",
    "        # del x3\n",
    "        x = self.up3(x)\n",
    "\n",
    "        x2 = self.one_conv2(x2)\n",
    "        x = torch.cat((x, x2), dim=1)\n",
    "        # del x2\n",
    "        x = self.up4(x)\n",
    "\n",
    "        x1 = self.one_conv1(x1)\n",
    "        x = torch.cat((x, x1), dim=1)\n",
    "        # del x1\n",
    "        x = self.up5(x)\n",
    "\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "    def predict_mask(self, query, target):\n",
    "        return self.forward(query, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T11:35:51.483214Z",
     "start_time": "2020-12-18T11:35:51.480773Z"
    }
   },
   "outputs": [],
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T11:35:55.588428Z",
     "start_time": "2020-12-18T11:35:53.442206Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import functools, operator, collections\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import jaccard_score as jsc\n",
    "from sklearn.metrics import average_precision_score as avg_pr\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "\n",
    "def eval_net(model,\n",
    "         loader, \n",
    "         device, \n",
    "         bbox: bool, \n",
    "         verbose: bool,\n",
    "         iou_thr: int = 0.5\n",
    "         ):\n",
    "#     logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s: %(message)s\", filename='oneshot_eval.log')\n",
    "    model.eval()\n",
    "\n",
    "    # Number of batches\n",
    "    n_val = len(loader)\n",
    "\n",
    "    logging.info(f\"n_val: {n_val}\")\n",
    "    \n",
    "    matches = 0\n",
    "\n",
    "    # Number of bboxes\n",
    "    max_matches = 0\n",
    "\n",
    "    if bbox:\n",
    "        truth_type = \"bbox\"\n",
    "    else:\n",
    "        truth_type = \"mask\"\n",
    "    \n",
    "    logging.info(f\"type of ground truth: {truth_type}\")\n",
    "\n",
    "    precisions, recalls, accuracies = [], [], []\n",
    "\n",
    "    batch_results = []\n",
    "\n",
    "    with tqdm(total=n_val, desc='Validation round', unit='img', leave=False, disable=not verbose) as bar:\n",
    "        for batch in loader:\n",
    "            logging.info(f\"New batch!\")\n",
    "            queries, targets, truth = batch['query'], batch['target'], batch[truth_type]\n",
    "            queries = queries.to(device=device, dtype=torch.float32)\n",
    "            targets = targets.to(device=device, dtype=torch.float32)\n",
    "            # truth = truth.to(device=device, dtype=torch.float32)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = model(queries, targets)\n",
    "                logging.info(f\"Masks predicted\")\n",
    "                pred_masks = pred.cpu().numpy()\n",
    "                logging.info(f\"Masks to CPU\")\n",
    "                print(f\"pred_masks_shape_0: {pred_masks.shape[0]}\")\n",
    "                print(f\"pred_masks: {pred_masks}\")\n",
    "                # assunzione: gli indici della true e pred masks sono gli stessi\n",
    "                for mask_index in range(pred_masks.shape[0]):\n",
    "                    pred_mask = np.asarray(pred_masks[mask_index])\n",
    "                    pred_mask = masks_as_image(rle_encode(pred_mask))\n",
    "                    \n",
    "                    # mask labeling and conversion to bboxes\n",
    "                    pred_labels = label(pred_mask)\n",
    "                    pred_bboxes_coords = np.array(list(map(lambda x: x.bbox, regionprops(pred_labels))))\n",
    "                    pred_bboxes = calc_bboxes_from_coords(pred_bboxes_coords)\n",
    "\n",
    "                    # computes truth bboxes in the same way as the pred\n",
    "                    if bbox:\n",
    "                        truth_bboxes = np.array(truth[mask_index])\n",
    "                    else:\n",
    "                        truth_mask = np.asarray(truth[mask_index])\n",
    "                        truth_mask = masks_as_image(rle_encode(truth_mask))\n",
    "                        true_mask_labels = label(truth_mask)\n",
    "                        truth_bboxes_coords = np.array(list(map(lambda x: x.bbox, regionprops(true_mask_labels))))\n",
    "                        truth_bboxes = calc_bboxes_from_coords(truth_bboxes_coords)\n",
    "\n",
    "                    max_matches += len(truth_bboxes)\n",
    "                    \n",
    "                    b_result = get_pred_results(truth_bboxes, pred_bboxes, iou_thr)\n",
    "                    print(f\"b_result: {b_result}\")\n",
    "                    batch_results.append(b_result)\n",
    "                \n",
    "                logging.info(f\"Batch finished. batch_results: {batch_results}\")\n",
    "\n",
    "    print(f\"Validation from eval completed\")\n",
    "    # Should not be here, since the eval method is used in both validation and test -> TODO: better handling of the flag.\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"Batch results: {batch_results}\")\n",
    "    \n",
    "    result = dict(functools.reduce(operator.add, map(collections.Counter, batch_results)))\n",
    "    print(\"Validation output: \", str(result))\n",
    "    true_pos = result['true_pos']\n",
    "    false_pos = result['false_pos']\n",
    "    false_neg = result['false_neg']\n",
    "\n",
    "    precision = calc_precision(true_pos, false_pos)\n",
    "    recall = calc_recall(true_pos, false_neg)\n",
    "    accuracy = calc_accuracy(true_pos, false_pos, false_neg)\n",
    "    print(f\"Precision: {precision}    Recall: {recall}    Accuracy: {accuracy}\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def calc_bboxes_from_coords(bboxes_coords):\n",
    "    \"\"\"Calculate all bounding boxes from a set of bounding boxes coordinates\"\"\"\n",
    "    bboxes = []\n",
    "    for coord_idx in range(len(bboxes_coords)):\n",
    "        coord = bboxes_coords[coord_idx]\n",
    "        bbox = (coord[1], coord[0], int(coord[4])-int(coord[1]), int(coord[3])-int(coord[0]))\n",
    "        bboxes.append(bbox)\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def get_pred_results(truth_bboxes, pred_bboxes, iou_thr = 0.5):\n",
    "    \"\"\"Calculates true_pos, false_pos and false_neg from the input bounding boxes. \"\"\"\n",
    "    n_pred_idxs = range(len(pred_bboxes))\n",
    "    n_truth_idxs = range(len(truth_bboxes))\n",
    "    if len(n_pred_idxs) == 0:\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = len(truth_bboxes)\n",
    "        return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}\n",
    "    if len(n_truth_idxs) == 0:\n",
    "        tp = 0\n",
    "        fp = len(pred_bboxes)\n",
    "        fn = 0\n",
    "        return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}\n",
    "\n",
    "    truth_idx_thr = []\n",
    "    pred_idx_thr = []\n",
    "    ious = []\n",
    "    for pred_idx, pred_bbox in enumerate(pred_bboxes):\n",
    "        for truth_idx, truth_bbox in enumerate(truth_bboxes):\n",
    "            iou = get_jaccard(pred_bbox, truth_bbox)\n",
    "            if iou > iou_thr:\n",
    "                truth_idx_thr.append(truth_idx)\n",
    "                pred_idx_thr.append(pred_idx)\n",
    "                ious.append(iou)\n",
    "    # ::-1 reverses the list\n",
    "    ious_desc = np.argsort(ious)[::-1]\n",
    "    if len(ious_desc) == 0:\n",
    "        # No matches\n",
    "        tp = 0\n",
    "        fp = len(pred_bboxes)\n",
    "        fn = len(truth_bboxes)\n",
    "    else:\n",
    "        truth_match_idxes = []\n",
    "        pred_match_idxes = []\n",
    "        for idx in ious_desc:\n",
    "            truth_idx = truth_idx_thr[idx]\n",
    "            pred_idx = pred_idx_thr[idx]\n",
    "            # If the bboxes are unmatched, add them to matches\n",
    "            if (truth_idx not in truth_match_idxes) and (pred_idx not in pred_match_idxes):\n",
    "                truth_match_idxes.append(truth_idx)\n",
    "                pred_match_idxes.append(pred_match_idxes)\n",
    "        tp = len(truth_match_idxes)\n",
    "        fp = len(pred_bboxes) - len(pred_match_idxes)\n",
    "        fn = len(truth_bboxes) - len(truth_match_idxes)\n",
    "    return {'true_pos': tp, 'false_pos': fp, 'false_neg': fn}\n",
    "\n",
    "\n",
    "def calc_precision(true_pos, false_neg):\n",
    "    try:\n",
    "        precision = true_pos / (true_pos + false_neg)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0.0\n",
    "    return precision\n",
    "\n",
    "\n",
    "def calc_recall(true_pos, false_pos):\n",
    "    try:\n",
    "        recall = true_pos / (true_pos + false_pos)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "    return recall\n",
    "\n",
    "\n",
    "def calc_accuracy(true_pos, false_pos, false_neg):\n",
    "    try:\n",
    "        accuracy = true_pos / (true_pos + false_pos + false_neg)\n",
    "    except ZeroDivisionError:\n",
    "        accuracy = 0.0\n",
    "    return accuracy \n",
    "\n",
    "\n",
    "def calc_mavg_precision(precision_array):\n",
    "    return \n",
    "\n",
    "\n",
    "def get_jaccard(pred_bbox, truth_bbox):\n",
    "    pred_mask = get_mask_from_bbox(pred_bbox)\n",
    "    truth_mask = get_mask_from_bbox(truth_bbox)\n",
    "    return get_jaccard_from_mask(pred_mask, truth_mask)\n",
    "\n",
    "\n",
    "def get_jaccard_from_mask(pred_masks, truth):\n",
    "    return jsc(pred_masks, truth)\n",
    "\n",
    "\n",
    "def get_mask_from_bbox(bbox):\n",
    "    mask = np.zeros((256, 256))\n",
    "    x = bbox[0]\n",
    "    y = bbox[1]\n",
    "    for width in range(bbox[2] + 1):\n",
    "        for height in range(bbox[3] + 1):\n",
    "            mask[x + width, y + height] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_bbox_batch(img):\n",
    "    bbox = np.empty((img.shape[0], 4))\n",
    "    for i in range(img.shape[0]):\n",
    "        bbox[i] = get_bbox(img[i])\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def get_bbox(img):\n",
    "    # img.shape = [batch_size, 1, 256, 256]\n",
    "    rows = np.any(img, axis=-1)\n",
    "    cols = np.any(img, axis=-2)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    # X, Y, Width, Height\n",
    "    return [cmin, rmin, cmax-cmin, rmax-rmin]\n",
    "\n",
    "\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels_old = img.T.flatten()\n",
    "    pixels = img.T.flatten()\n",
    "    for x in range(len(pixels_old)):\n",
    "        if pixels_old[x] > 0.5:\n",
    "            pixels[x] = 1\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle, shape=(256, 256)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "\n",
    "def masks_as_image(in_mask_list, all_masks=None):\n",
    "    \"\"\"\n",
    "    Take the complete rle_encoded mask and create a mask array of the single masks\n",
    "    \"\"\"\n",
    "    if all_masks is None:\n",
    "        all_masks = np.zeros((256, 256), dtype=np.int16)\n",
    "    # if isinstance(in_mask_list, list):\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return np.expand_dims(all_masks, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T11:35:55.595380Z",
     "start_time": "2020-12-18T11:35:55.591809Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T12:02:10.007656Z",
     "start_time": "2020-12-18T12:02:08.695691Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "import h5py\n",
    "import tables\n",
    "\n",
    "\n",
    "# TODO: Deve preprocessare anche le immagini di test\n",
    "# TODO: Ha da funzionà co TorchVision, se hai tempo\n",
    "# TODO: Visto che le maschere ci servono solo per estrapolare le bbox e confrontarle con quelle stimate, ha senso portarsi tutta la maschera e non le singole bbox?\n",
    "class BasicDataset(Dataset):\n",
    "    TARGET_IMAGE_PATH = \"target_image_path\"\n",
    "    MASK_IMAGE_PATH = \"mask_image_path\"\n",
    "    BBOX_PATH = \"bbox_path\"\n",
    "    TARGET_IMAGE_BBOX_PATH = \"target_image_bbox_path\"\n",
    "\n",
    "    # TODO: Check if the values are empty\n",
    "    def __init__(self, imgs_dir: str, masks_dir: str, dataset_name: str, mask_image_dim: int = 256, query_dim: int = 64,\n",
    "                 bbox_suffix: str = '.bboxes.txt', save_to_disk: bool = False, skip_bbox_lines: int = 0):\n",
    "        self.imgs_dir = fix_input_dir(imgs_dir)\n",
    "        self.masks_dir = fix_input_dir(masks_dir)\n",
    "        self.processed_img_dir = str(self.imgs_dir[:self.imgs_dir.rindex(os.path.sep) + 1]) + \"preprocessed\"\n",
    "        self.mask_img_dim = mask_image_dim\n",
    "        self.query_dim = query_dim\n",
    "        self.bbox_suffix = bbox_suffix\n",
    "        self.save_to_disk = save_to_disk\n",
    "        self.skip_bbox_lines = skip_bbox_lines\n",
    "        assert mask_image_dim > 1, 'The dimension of mask and image must be higher than 1'\n",
    "        assert query_dim > 1, 'The dimension of query image must be higher than 1'\n",
    "\n",
    "        assert os.path.isdir(imgs_dir), f\"Bad path for images directory: {imgs_dir}\"\n",
    "\n",
    "        assert os.path.isdir(masks_dir), f\"Bad path for masks directory: {masks_dir}\"\n",
    "\n",
    "        if save_to_disk:\n",
    "            # create processed image's directory, if not exists yet\n",
    "            try:\n",
    "                os.mkdir(self.processed_img_dir)\n",
    "            except FileExistsError:\n",
    "                # some previous instance generate this directory, no need to raise an exception\n",
    "                pass\n",
    "\n",
    "        # list of dict with the path of the images, which contains the paths for the following images:\n",
    "        #       target, mask, bbox, target's bbox\n",
    "        # every dict is defined by 4 str keys which have a str value\n",
    "        # key = type of image\n",
    "        # value = path of the image\n",
    "\n",
    "        # List of dict. Every dict refers to an image with 4 keys:\n",
    "        #       target, mask, bbox, query_target_bbox\n",
    "        self.images_path = []\n",
    "\n",
    "        # TODO: Fai in modo che preprocess calcoli sia la maschera che il bbox e poi, in base al dataset, togline uno\n",
    "        if dataset_name == \"FlickrLogos-32\":\n",
    "            self.flickrlogos32_load()\n",
    "        elif dataset_name == \"TopLogos-10\":\n",
    "            self.toplogos10_load()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "    def toplogos10_load(self):\n",
    "\n",
    "        # get bbox path\n",
    "        bbox_path = None\n",
    "        for bbox_paths, _, bbox_list in os.walk(self.masks_dir):\n",
    "            for bbox_file in bbox_list:\n",
    "                if self.bbox_suffix in bbox_file:\n",
    "                    bbox_path = get_class_file_path(bbox_paths, bbox_file)\n",
    "                    break\n",
    "            if bbox_path:\n",
    "                break\n",
    "\n",
    "        # get query image path\n",
    "        query_full_image_path = f\"{bbox_path[:bbox_path.index(self.bbox_suffix)]}\"\n",
    "\n",
    "        # get target images path and fill \"self.images_path\"\n",
    "        for target_images_paths, _, target_images_list in os.walk(self.imgs_dir):\n",
    "            for target_image_name in target_images_list:\n",
    "                target_image_root_path, target_image_extension = os.path.splitext(\n",
    "                    os.path.join(target_images_paths, target_image_name))\n",
    "                if target_image_extension == \".jpg\":\n",
    "                    self.images_path.append(\n",
    "                        {self.TARGET_IMAGE_PATH: get_class_file_path(target_images_paths, target_image_name),\n",
    "                         self.MASK_IMAGE_PATH: None,\n",
    "                         self.BBOX_PATH: bbox_path,\n",
    "                         self.TARGET_IMAGE_BBOX_PATH: query_full_image_path})\n",
    "\n",
    "    def flickrlogos32_load(self):\n",
    "\n",
    "        # dict with merged masks path\n",
    "        # key = target image file name\n",
    "        # value = merged mask's file path\n",
    "        masks_dict = {}\n",
    "\n",
    "        # put stuff into masks_dict\n",
    "        for masks_paths, _, masks_files in os.walk(self.masks_dir):\n",
    "            for mask_file_name in masks_files:\n",
    "                _, mask_extension = os.path.splitext(os.path.join(masks_paths, mask_file_name))\n",
    "                if mask_extension == \".png\" and \"merged\" in mask_file_name:\n",
    "                    masks_dict[mask_file_name[:mask_file_name.rindex(\".mask\")]] = get_class_file_path(masks_paths,\n",
    "                                                                                                      mask_file_name)\n",
    "\n",
    "        # dict with every image of every class\n",
    "        # key = class name\n",
    "        # value = dict with images type and path\n",
    "        #       key = type of image (target, query, mask)\n",
    "        #       value = path of the file\n",
    "        image_path_element = {}\n",
    "\n",
    "        # put stuff into image_path_element\n",
    "        for target_images_paths, _, target_images_files in os.walk(self.imgs_dir):\n",
    "            # TODO: Compare come classe la cartella padre \"jpg\", trova un modo per risolvere\n",
    "            target_image_class = target_images_paths[target_images_paths.rindex(os.path.sep) + 1:]\n",
    "            for target_image_name in target_images_files:\n",
    "                target_image_root_path, target_image_extension = os.path.splitext(\n",
    "                    os.path.join(target_images_paths, target_image_name))\n",
    "                if target_image_extension == \".jpg\" and \"no-logo\" not in target_image_root_path:\n",
    "                    x = {self.TARGET_IMAGE_PATH: get_class_file_path(target_images_paths, target_image_name),\n",
    "                         self.MASK_IMAGE_PATH: masks_dict[target_image_name],\n",
    "                         self.BBOX_PATH: f'{masks_dict[target_image_name][:masks_dict[target_image_name].rindex(\".mask\")]}{self.bbox_suffix}'}\n",
    "                    try:\n",
    "                        image_path_element[target_image_class].append(x)\n",
    "                    except KeyError:\n",
    "                        image_path_element[target_image_class] = [x]\n",
    "\n",
    "        # fill \"images_path\" variable. it generate every couple (target image, query image) for the same class\n",
    "        # for now, it only skips couple (target, query) of the same image\n",
    "        for target_image_class in image_path_element:\n",
    "            items_class = image_path_element[target_image_class]\n",
    "            for outer_image in items_class:\n",
    "                bbox_path = outer_image[self.BBOX_PATH]\n",
    "                outer_target_image_path = outer_image[self.TARGET_IMAGE_PATH]\n",
    "                for inner_image in items_class:\n",
    "                    if not outer_image == inner_image:\n",
    "                        target_image_path = inner_image[self.TARGET_IMAGE_PATH]\n",
    "                        mask_image_path = inner_image[self.MASK_IMAGE_PATH]\n",
    "                        self.images_path.append({self.TARGET_IMAGE_PATH: target_image_path,\n",
    "                                                 self.MASK_IMAGE_PATH: mask_image_path,\n",
    "                                                 self.BBOX_PATH: bbox_path,\n",
    "                                                 self.TARGET_IMAGE_BBOX_PATH: outer_target_image_path})\n",
    "        print(f\"You have {len(self.images_path)} triplets\")\n",
    "\n",
    "    # preprocess the images. then save in file and return a list triplet [query image, target image, mask image]. how?\n",
    "    # stretch the target image\n",
    "    # stretch, crop and stretch again the query image\n",
    "    # stretch the mask image\n",
    "    # TODO: Check if the values are empty\n",
    "    @classmethod\n",
    "    def preprocess(cls, target_img_path: str, bbox_path: str, query_full_img_path: str, skip_bbox_lines: int = 0,\n",
    "                   img_dim: int = 256, query_img_dim: int = 64, mask_img_path: str = None) -> dict:\n",
    "\n",
    "        # Target image\n",
    "\n",
    "        pil_target_img = Image.open(target_img_path)\n",
    "        # stretch the image\n",
    "        pil_resized_target_img = pil_target_img.resize((img_dim, img_dim))\n",
    "\n",
    "        # Query image\n",
    "\n",
    "        pil_target_img_bbox = Image.open(query_full_img_path)\n",
    "        pil_resized_target_img_bbox = pil_target_img_bbox.resize((img_dim, img_dim))\n",
    "\n",
    "        # we will resize, crop and resize again the image but we have the coordinates of the non resized bounding box\n",
    "        target_img_width, target_img_height = pil_target_img_bbox.size\n",
    "        resized_target_img_width, resized_target_img_height = pil_resized_target_img_bbox.size\n",
    "        percent_width = round(100 * int(resized_target_img_width) / (int(target_img_width)), 2) / 100\n",
    "        percent_height = round(100 * int(resized_target_img_height) / (int(target_img_height)), 2) / 100\n",
    "\n",
    "        # open the bounding box file\n",
    "        with open(bbox_path) as bbox_file:\n",
    "            # read only the first line of the bbox file\n",
    "            bbox_lines = bbox_file.readlines()\n",
    "            first_line_bbox = bbox_lines[1 - skip_bbox_lines].split(' ')\n",
    "            # check if we correctly skipped the first line of the file, the one with no number,\n",
    "            # and if all the elements are numeric, like every coordinate should be ;)\n",
    "            if first_line_bbox[0].isnumeric() and first_line_bbox[1].isnumeric() and \\\n",
    "                    first_line_bbox[2].isnumeric() and first_line_bbox[3].rstrip().isnumeric():\n",
    "                x, y, width, height = first_line_bbox\n",
    "                # adapt the old coordinates to the new stretched dimension\n",
    "                left = int(x.strip()) * percent_width\n",
    "                upper = int(y.strip()) * percent_height\n",
    "                right = int(int(x.strip()) + int(width)) * percent_width\n",
    "                lower = int(int(y.strip()) + int(height)) * percent_height\n",
    "                # crop and resize the query image\n",
    "                pil_query_img = pil_resized_target_img_bbox.crop((left, upper, right, lower))\n",
    "                pil_resized_query_img = pil_query_img.resize((query_img_dim, query_img_dim))\n",
    "            else:\n",
    "                # TODO: nel traceback compare \"error_string\" e poi successivamente spiega l'eccezione. Trova un modo per togliere quel \"error_string\"\n",
    "                error_string = f\"Bounding box file's first line should have 4 groups of integers with whitespace \" \\\n",
    "                               f\"separator. Check {bbox_path}\"\n",
    "                raise Exception(error_string)\n",
    "\n",
    "        # Mask\n",
    "\n",
    "        if mask_img_path:\n",
    "            pil_mask = Image.open(mask_img_path)\n",
    "            pil_resized_mask = pil_mask.resize((img_dim, img_dim))\n",
    "        else:\n",
    "            pil_resized_mask = None\n",
    "\n",
    "        # get the size of the images\n",
    "        # print(f\"query image dim: {pil_resized_query_img.size}\")\n",
    "        # print(f\"target image dim: {pil_resized_target_img.size}\")\n",
    "        # if pil_resized_mask:\n",
    "        #     print(f\"mask image dim: {pil_resized_mask.size}\")\n",
    "\n",
    "        # just to test if everything works. don't look at these :)\n",
    "        # if pil_resized_target_img:\n",
    "        #     pil_resized_target_img.save('target.jpg')\n",
    "        # if pil_resized_query_img:\n",
    "        #     pil_resized_query_img.save('query.jpg')\n",
    "        # if pil_resized_mask:\n",
    "        #     pil_resized_mask.save('mask.jpg')\n",
    "\n",
    "        # return the triplet (Dq, Dt, Dm) where Dq is the query image, Dt is the target image and Dm is the mask image\n",
    "        return create_triplet_with_torch_representation(pil_resized_query_img,\n",
    "                                                        pil_resized_target_img,\n",
    "                                                        pil_resized_mask)\n",
    "\n",
    "    # def h5py_with_pytorch(self, pil_img, index, type):\n",
    "    #     x = self.h5py_compression(to_pytorch(pil_img), index, type)\n",
    "    #     return x\n",
    "    #\n",
    "    # def h5py_without_pytorch(self, pil_img, index, type):\n",
    "    #     x = self.h5py_compression(pil_img, index, type)\n",
    "    #     return x\n",
    "\n",
    "    # def store_hdf5_file_with_compression(self, image, image_index, image_type):\n",
    "    #     file_name = f'{self.processed_img_dir}{os.path.sep}{image_index}_{image_type}.hdf5'\n",
    "    #     f = h5py.File(file_name, \"w\")\n",
    "    #     # TODO: Esistono altri algoritmi di compressione come Mafisc. Una roba figa che puoi usare è Bitshuffle\n",
    "    #     f.create_dataset(\"init\", compression=\"gzip\", compression_opts=9, data=image)\n",
    "    #     f.close()\n",
    "    #     return file_name\n",
    "\n",
    "    def store_hdf5_file_with_compression(self, images, image_index):\n",
    "        image_type = [\"query\", \"target\", \"mask\"]\n",
    "        image_type_index = 0\n",
    "        for image in images:\n",
    "            file_name = f'{self.processed_img_dir}{os.path.sep}{image_index}_{image}.hdf5'\n",
    "            f = h5py.File(file_name, \"w\")\n",
    "            # TODO: Esistono altri algoritmi di compressione come Mafisc. Una roba figa che puoi usare è Bitshuffle\n",
    "            f.create_dataset(\"init\", compression=\"gzip\", compression_opts=9, data=images[image])\n",
    "            f.close()\n",
    "            image_type_index += 1\n",
    "        # for image in images:\n",
    "        #     file_name = f'{self.processed_img_dir}{os.path.sep}{image_index}_{image_type[image_type_index]}.hdf5'\n",
    "        #     f = h5py.File(file_name, \"w\")\n",
    "        #     # TODO: Esistono altri algoritmi di compressione come Mafisc. Una roba figa che puoi usare è Bitshuffle\n",
    "        #     f.create_dataset(\"init\", compression=\"gzip\", compression_opts=9, data=image)\n",
    "        #     f.close()\n",
    "        #     image_type_index += 1\n",
    "        # return file_name\n",
    "\n",
    "    # def gzip_compress(self, index, input_file):\n",
    "    #     # input_file = f'{self.processed_img_dir}{os.path.sep}{index}.npz'\n",
    "    #     with open(input_file, 'rb') as f_in:\n",
    "    #         output_file = f'{input_file}.gz'\n",
    "    #         with gzip.open(output_file, 'wb', compresslevel=9) as f_out:\n",
    "    #             shutil.copyfileobj(f_in, f_out)\n",
    "    #     # if os.path.exists(input_file):\n",
    "    #     #     os.remove(input_file)\n",
    "    #     return output_file\n",
    "\n",
    "    # def gzip_compress(self, index, input_file):\n",
    "    #     # input_file = f'{self.processed_img_dir}{os.path.sep}{index}.npz'\n",
    "    #     output_file = f'{input_file}.gz'\n",
    "    #     with gzip.open(output_file, 'wb', compresslevel=1) as f_out:\n",
    "    #         with open(input_file, 'rb') as f_in:\n",
    "    #             shutil.copyfileobj(f_in, f_out)\n",
    "    #     # if os.path.exists(input_file):\n",
    "    #     #     os.remove(input_file)\n",
    "    #     return output_file\n",
    "\n",
    "    # def gzip_uncompress(self, input_file):\n",
    "    #     with gzip.open(input_file, 'rb') as f:\n",
    "    #         file_content = f.read()\n",
    "    #     output_file = input_file[:input_file.rindex('.')]\n",
    "    #     with open(output_file, mode='wb') as fp:\n",
    "    #         fp.write(file_content)\n",
    "    #     return output_file\n",
    "\n",
    "    def read_hdf5_file(self, hdf5_file):\n",
    "        with h5py.File(hdf5_file, 'r') as hf:\n",
    "            data = hf.get('init')\n",
    "            data = np.array(data)\n",
    "        return data\n",
    "\n",
    "    # def np_save_compressed(self, index, triplet_list_in_torch_representation):\n",
    "    #     file_name = f'{self.processed_img_dir}{os.path.sep}{index}'\n",
    "    #     # save the file so the next time you don't have to preprocess again\n",
    "    #     np.savez_compressed(file_name,\n",
    "    #                         query=triplet_list_in_torch_representation[0],\n",
    "    #                         target=triplet_list_in_torch_representation[1],\n",
    "    #                         mask=triplet_list_in_torch_representation[2])\n",
    "    #     return f'{file_name}.npz'\n",
    "\n",
    "    def __getitem__(self, item_index):\n",
    "#         print(f\"Getting item {item_index}\")\n",
    "        # get the path of the preprocessed file, if exists\n",
    "        mask_file_path = f'{self.processed_img_dir}{os.path.sep}{item_index}_mask.hdf5'\n",
    "        query_file_path = f'{self.processed_img_dir}{os.path.sep}{item_index}_query.hdf5'\n",
    "        target_file_path = f'{self.processed_img_dir}{os.path.sep}{item_index}_target.hdf5'\n",
    "\n",
    "        correct_order_triplet = [query_file_path, target_file_path, mask_file_path]\n",
    "        triplet_element_order = [\"query\", \"target\", \"mask\"]\n",
    "\n",
    "        return_dict = {}\n",
    "        for file in correct_order_triplet:\n",
    "            if not os.path.exists(file):\n",
    "                # triplet = self.preprocess(item_index, self.images_path[item_index])\n",
    "                return_dict = self.preprocess(\n",
    "                    target_img_path=get_full_path(self.imgs_dir, self.images_path[item_index][self.TARGET_IMAGE_PATH]),\n",
    "                    bbox_path=get_full_path(self.masks_dir, self.images_path[item_index][self.BBOX_PATH]),\n",
    "                    query_full_img_path=get_full_path(self.imgs_dir,\n",
    "                                                      self.images_path[item_index][self.TARGET_IMAGE_BBOX_PATH]),\n",
    "                    mask_img_path=get_full_path(self.masks_dir, self.images_path[item_index][self.MASK_IMAGE_PATH]),\n",
    "                    skip_bbox_lines=self.skip_bbox_lines)\n",
    "                if self.save_to_disk:\n",
    "                    self.store_hdf5_file_with_compression(return_dict, item_index)\n",
    "                break\n",
    "        else:\n",
    "            triplet_index = 0\n",
    "            for file in correct_order_triplet:\n",
    "                if os.path.exists(file):\n",
    "                    hdf5_file = self.read_hdf5_file(file)\n",
    "                    return_dict[triplet_element_order[triplet_index]] = to_pytorch(hdf5_file)\n",
    "                triplet_index += 1\n",
    "#         print(f\"Ciao, sono un fantastico {return_dict}\")\n",
    "        return return_dict\n",
    "\n",
    "        # for file in correct_order_triplet:\n",
    "        # # check if preprocessed file exists. if not, he will generate it. then return the triplet\n",
    "        #     if os.path.exists(file):\n",
    "        #         data = np.load(file, mmap_mode='r')\n",
    "        #         return_tuple = np.array([data['query'], data['target'], data['mask']])\n",
    "        #     else:\n",
    "        #         return_tuple = self.preprocess(i, self.images_path[i])\n",
    "        # return return_tuple\n",
    "\n",
    "\n",
    "# something that will be deleted\n",
    "# class CarvanaBasicDataset(BasicDataset):\n",
    "#     def __init__(self, imgs_dir, masks_dir, scale=1):\n",
    "#         super().__init__(imgs_dir, masks_dir, scale, mask_suffix='_mask')\n",
    "\n",
    "\n",
    "def to_pytorch(image):\n",
    "    if image:\n",
    "        image_np = np.array(image)\n",
    "        # mask image has only one channel, we need to explicit it\n",
    "        if len(image_np.shape) == 2:\n",
    "            image_np = np.expand_dims(image_np, axis=2)\n",
    "        # HWC to CHW for pytorch\n",
    "        img_trans = image_np.transpose((2, 0, 1))\n",
    "        if img_trans.max() > 1:\n",
    "            img_trans = img_trans / 255\n",
    "        return torch.from_numpy(img_trans).type(torch.FloatTensor)\n",
    "        # return img_trans\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# dude, the name says all. just read it :/\n",
    "# def create_triplet_without_torch_representation(pil_query, pil_target, pil_mask):\n",
    "#     # return [np.array(pil_query), np.array(pil_target), np.array(pil_mask)]\n",
    "#     # return np.array([np.array(pil_query), np.array(pil_target), np.array(pil_mask)])\n",
    "#     return {\n",
    "#         \"query\": np.array(pil_query),\n",
    "#         \"target\": np.array(pil_target),\n",
    "#         \"mask\": np.array(pil_mask)\n",
    "#     }\n",
    "\n",
    "def create_triplet_with_torch_representation(pil_query, pil_target, pil_mask):\n",
    "    # return [to_pytorch(pil_query), to_pytorch(pil_target), to_pytorch(pil_mask)]\n",
    "    # return np.array([to_pytorch(pil_query), to_pytorch(pil_target), to_pytorch(pil_mask)])\n",
    "    return {\n",
    "        \"query\": to_pytorch(pil_query),\n",
    "        \"target\": to_pytorch(pil_target),\n",
    "        \"mask\": to_pytorch(pil_mask)\n",
    "    }\n",
    "\n",
    "\n",
    "def get_class_file_path(class_name, file_name):\n",
    "    class_file = f\"{class_name[class_name.rindex(os.path.sep):]}{os.path.sep}{file_name}\"\n",
    "    if os.path.sep not in class_file[0:2]:\n",
    "        class_file = f\"{os.path.sep}{class_file}\"\n",
    "    return class_file\n",
    "\n",
    "\n",
    "def get_full_path(root, file):\n",
    "    try:\n",
    "        if root[root.rindex(os.path.sep) + 1:].strip() == file[1:file.index(os.path.sep, 1)].strip():\n",
    "            path = f\"{root[:root.rindex(os.path.sep)]}{file}\"\n",
    "        else:\n",
    "            path = f\"{root}{file}\"\n",
    "        return path\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fix_input_dir(dir):\n",
    "    if not dir.strip()[-1:] == os.path.sep:\n",
    "        return dir.strip()\n",
    "    return dir.strip()[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T12:02:10.015056Z",
     "start_time": "2020-12-18T12:02:10.011036Z"
    }
   },
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T12:02:11.231900Z",
     "start_time": "2020-12-18T12:02:10.718191Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nvidia/anaconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model.model import LogoDetection\n",
    "from utils.dataset_loader import BasicDataset\n",
    "\n",
    "# todo: when we add more models, we should move these variables to another location\n",
    "MODEL_HOME = os.path.abspath(\"./stored_models/\")\n",
    "ALL_MODEL_NAMES = [\"LogoDetection\"]\n",
    "ALL_DATASET_NAMES = [\"FlickrLogos-32\", \"TopLogos-10\"]\n",
    "\n",
    "with open(os.path.abspath(\"./config/config.yaml\")) as config:\n",
    "    config_list = yaml.load(config, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "# # Appl.load_aly Gaussian normalization to the model\n",
    "# def weights_init(model):\n",
    "#     if isinstance(model, nn.Module):\n",
    "#         nn.init.normal_(model.weight.data, mean=0.0, std=0.01)\n",
    "\n",
    "def train(model,\n",
    "          device,\n",
    "          train_loader,\n",
    "          val_loader,\n",
    "          max_epochs,\n",
    "          optimizer,\n",
    "          verbose,\n",
    "          checkpoint_dir,\n",
    "          model_path,\n",
    "          save_cp,\n",
    "          n_train,\n",
    "          n_val\n",
    "          ):\n",
    "    batch_size = train_loader.batch_size\n",
    "\n",
    "    # Logging for TensorBoard\n",
    "    writer = SummaryWriter(\n",
    "        comment=f'LR__BS_{batch_size}_OPT_{type(optimizer).__name__}')  # does optimizer.lr work? we're gonne find out\n",
    "    global_step = 0\n",
    "\n",
    "    ### ERROR: n_train e n_val? ###\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:             {max_epochs}\n",
    "        Batch size:         {batch_size}\n",
    "        Learning rate:      \n",
    "        Training size:      {n_train}\n",
    "        Validation size:    {n_val}\n",
    "        Device:             {device.type}\n",
    "    ''')\n",
    "\n",
    "    def criterion(pred, true):\n",
    "        return torch.div(nn.BCELoss()(pred, true), 256 * 256)  # L = (1/(H*W)) * BCELoss\n",
    "        # TypeError: unsupported operand type(s) for /: 'BCELoss' and 'int'\n",
    "\n",
    "    last_epoch_val_score = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        logging.info(f\"Epoch number #insert_epoch_number\")\n",
    "        model.train()  # set the model in training flag to True\n",
    "        epoch_loss = 0  # resets the loss for the current epoch\n",
    "        # epoch(batch_size, train_samples)\n",
    "\n",
    "        # TODO\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{max_epochs}', unit='img', disable=not verbose) as bar:\n",
    "            bar.set_description(f'train loss')\n",
    "\n",
    "            for batch in train_loader:\n",
    "#                 np.set_printoptions(threshold=sys.maxsize)\n",
    "#                 print(f\"Batch: {batch}\")\n",
    "                logging.info(f\"Batch number #\")\n",
    "                queries = batch['query']  # Correct dimensions?\n",
    "                targets = batch['target']\n",
    "                true_masks = batch['mask']\n",
    "\n",
    "                queries = queries.to(device=device, dtype=torch.float32)\n",
    "                targets = targets.to(device=device, dtype=torch.float32)\n",
    "                true_masks = true_masks.to(device=device, dtype=torch.float32)\n",
    "\n",
    "                logging.info(f\"Sending imgs to model\")\n",
    "                pred_masks = model(queries, targets)\n",
    "                logging.info(f\"Mask correctly predicted\")\n",
    "                # print(pred_masks.shape)\n",
    "                loss = criterion(pred_masks, true_masks)\n",
    "                epoch_loss += loss.detach().item()  # is the .detach() needed?\n",
    "\n",
    "                # TensorBoard logging\n",
    "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "\n",
    "                bar.set_postfix(loss=f'{loss.item():.5f}')\n",
    "                logging.info(f\"Loss: {loss.item()}\")\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # nn.utils.clip_grad_value_(net.parameters(), 0.1) Gradient Clipping\n",
    "                optimizer.step()\n",
    "\n",
    "                bar.update(queries.shape[0])\n",
    "                global_step += 1\n",
    "\n",
    "                # if n_train % batch_size == 0: \n",
    "                #     n_batch = n_train // batch_size\n",
    "                # else:\n",
    "                #     n_batch = n_train // batch_size + 1\n",
    "\n",
    "                ### DOMANDA: Dove lo volevamo usare? ###\n",
    "                ### ALTRA DOMANDA: Non conviene farlo fuori dai cicli? ###\n",
    "#                 n_batch = len(train_loader)\n",
    "                # Deve farlo sia in mezzo ai batch che a fine epoca. Modifica la condizione dell'if\n",
    "#                 if global_step % (n_train // (10 * batch_size)) == 0 or global_step == n_batch:\n",
    "#                     for tag, value in model.named_parameters():\n",
    "#                         tag = tag.replace('.', '/')\n",
    "#                         writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
    "#                         writer.add_histogram('grads/' + tag, value.grad.cpu().numpy(), global_step)\n",
    "#                     writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "#                     writer.add_images('query_images', queries, global_step)\n",
    "#                     writer.add_images('target_images', targets, global_step)\n",
    "#                     writer.add_images('masks/true', true_masks, global_step)\n",
    "#                     writer.add_images('masks/pred', pred_masks, global_step)\n",
    "\n",
    "            # TODO: Se save_cp è false e non viene cambiato il val_split di default non va in train il 10% del dataset. Si potrebbe fare in modo che non sia così\n",
    "            if save_cp:\n",
    "                logging.info(f\"Next operation is validation\")\n",
    "                print(f\"starting validation from train\")\n",
    "                val_score = eval_net(model, val_loader, device, bbox=False, verbose=True)\n",
    "                print(f\"validation completed\")\n",
    "                logging.info(f\"Validation complete\")\n",
    "                if val_score > last_epoch_val_score:\n",
    "                    try:\n",
    "                        os.mkdir(checkpoint_dir)\n",
    "                        logging.info('Created checkpoint directory')\n",
    "                    except OSError:  # Maybe FileExistsError ?\n",
    "                        pass\n",
    "                    model_files = [f for f in os.listdir(checkpoint_dir) if\n",
    "                                   os.path.isfile(os.path.join(checkpoint_dir, f))]\n",
    "                    torch.save(model.state_dict(), checkpoint_dir + f'CP_epoch{epoch + 1}.pt')\n",
    "                    for model_file in model_files:\n",
    "                        os.remove(f'{checkpoint_dir}{model_file}')\n",
    "                    logging.info(f'Checkpoint {epoch + 1} saved!')\n",
    "                    last_epoch_val_score = val_score\n",
    "\n",
    "    writer.close()\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # # WIP\n",
    "    # # Launches evaluation on the model every evaluate_every steps.\n",
    "    # # We need to change to appropriate evaluation metrics.\n",
    "    # if evaluate_every > 0 and valid_samples is not None and (e + 1) % evaluate_every == 0:\n",
    "    #     self.model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         mrr, h1 = self.evaluator.eval(samples=valid_samples, write_output= False)\n",
    "\n",
    "    #     # Metrics printing\n",
    "    #     print(\"\\tValidation: %f\" % h1)\n",
    "\n",
    "    # if save_path is not None:\n",
    "    #     print(\"\\tSaving model...\")\n",
    "    #     torch.save(self.model.state_dict(), save_path)\n",
    "    # print(\"\\tDone.\")\n",
    "\n",
    "\n",
    "# print(\"\\nEvaluating model...\")\n",
    "# model.eval()\n",
    "# mrr, h1 = Evaluator(model=model).eval(samples=dataset.test_samples, write_output=False)\n",
    "# print(\"\\tTest Hits@1: %f\" % h1)\n",
    "# print(\"\\tTest Mean Reciprocal Rank: %f\" % mrr)\n",
    "\n",
    "\n",
    "# def get_args():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--dataset',\n",
    "#                         choices=ALL_DATASET_NAMES,\n",
    "#                         default=\"FlickrLogos-32\",\n",
    "#                         type=str,\n",
    "#                         help=\"Dataset in {}\".format(ALL_DATASET_NAMES)\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--model',\n",
    "#                         choices=ALL_MODEL_NAMES,\n",
    "#                         default=\"LogoDetection\",\n",
    "#                         type=str,\n",
    "#                         help=\"Model in {}\".format(ALL_MODEL_NAMES)\n",
    "#                         )\n",
    "\n",
    "#     optimizers = ['Adam', 'SGD']\n",
    "#     parser.add_argument('--optimizer',\n",
    "#                         choices=optimizers,\n",
    "#                         default='Adam',\n",
    "#                         help=\"Optimizer in {}\".format(optimizers)\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--max_epochs',\n",
    "#                         default=500,\n",
    "#                         type=int,\n",
    "#                         help=\"Number of epochs\"\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--valid',\n",
    "#                         default=-1,\n",
    "#                         type=float,\n",
    "#                         help=\"Number of epochs before valid\"\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--batch_size',\n",
    "#                         default=32,\n",
    "#                         type=int,\n",
    "#                         help=\"Number of samples in each mini-batch in SGD and Adam optimization\"\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--weight_decay',\n",
    "#                         default=5e-4,\n",
    "#                         type=float,\n",
    "#                         help=\"L2 weight regularization of the optimizer\"\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--learning_rate',\n",
    "#                         default=4e-4,\n",
    "#                         type=float,\n",
    "#                         help=\"Learning rate of the optimizer\"\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--label_smooth',\n",
    "#                         default=0.1,\n",
    "#                         type=float,\n",
    "#                         help=\"Label smoothing for true labels\"\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--decay1',\n",
    "#                         default=0.9,\n",
    "#                         type=float,\n",
    "#                         help=\"Decay rate for the first momentum estimate in Adam\"\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--decay2',\n",
    "#                         default=0.999,\n",
    "#                         type=float,\n",
    "#                         help=\"Decay rate for second momentum estimate in Adam\"\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--verbose',\n",
    "#                         default=True,\n",
    "#                         type=bool,\n",
    "#                         help=\"Verbose\"\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--load',\n",
    "#                         type=str,\n",
    "#                         required=False,\n",
    "#                         help=\"Path to the model to load\"\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--batch_norm',\n",
    "#                         default=False,\n",
    "#                         type=bool,\n",
    "#                         help=\"If True, apply batch normalization\",\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--vgg_cfg',\n",
    "#                         type=str,\n",
    "#                         default='A',\n",
    "#                         help=\"VGG architecture config\",\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--step_eval',\n",
    "#                         type=int,\n",
    "#                         default=0,\n",
    "#                         help=\"Enables automatic evaluation checks every X step\",\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--val_split',\n",
    "#                         type=float,\n",
    "#                         default=0.1,\n",
    "#                         help=\"Forces the validation subset to be split according to the set value. Must a value in the [0-1] or the sofware WILL break\",\n",
    "#                         )\n",
    "\n",
    "#     parser.add_argument('--save_cp',\n",
    "#                         type=bool,\n",
    "#                         default=True,\n",
    "#                         help=\"If True, saves model checkponts\",\n",
    "#                         )\n",
    "\n",
    "#     return parser.parse_args()\n",
    "\n",
    "\n",
    "def train_main(dataset=\"FlickrLogos-32\",\n",
    "     model=\"LogoDetection\",\n",
    "     optimizer=\"Adam\",\n",
    "     max_epochs=1, \n",
    "     batch_size=32, \n",
    "     weight_decay=5e-4,\n",
    "     learning_rate=4e-4,\n",
    "     decay1=0.9, \n",
    "     decay2=0.999,\n",
    "     verbose=True,\n",
    "     batch_norm=False,\n",
    "     load=None,\n",
    "     val_split=0.1,\n",
    "     save_cp=True,\n",
    "     ):\n",
    "    # TODO: Add filename\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s: %(message)s\", filename='oneshot.log')\n",
    "#     args = get_args()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'Using device {device}')\n",
    "\n",
    "    # Modularized paths with respect to the current Dataset\n",
    "    imgs_dir = config_list['datasets'][dataset]['paths']['images']\n",
    "    masks_dir = config_list['datasets'][dataset]['paths']['masks']\n",
    "    checkpoint_dir = config_list['models'][model]['paths']['train_cp']\n",
    "\n",
    "    model_path = config_list['models'][model]['paths']['model'] + \"_\".join([model, dataset]) + \".pt\"\n",
    "\n",
    "    print(\"Loading %s dataset...\" % dataset)\n",
    "    # you can delete this \"save_to_disk\" to preserve the ssd :like:\n",
    "    dataset = BasicDataset(imgs_dir=imgs_dir, masks_dir=masks_dir, dataset_name=dataset)\n",
    "\n",
    "    # Splitting dataset\n",
    "    n_val = int(len(dataset) * val_split)\n",
    "    n_train = len(dataset) - n_val\n",
    "    # TODO: Il validation set dovrebbe avere il 10% di ogni classe e non il 10% del totale altrimenti verrebbe sbilanciato\n",
    "    train_set, val_set = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "    # Loading dataset\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True,\n",
    "                            drop_last=True)\n",
    "\n",
    "    # Change here to adapt your data\n",
    "    print(\"Initializing model...\")\n",
    "    model = LogoDetection(batch_norm=batch_norm)\n",
    "\n",
    "    # Optimizer selection\n",
    "    # build all the supported optimizers using the passed params (learning rate and decays if Adam)\n",
    "    supported_optimizers = {\n",
    "        'Adam': optim.Adam(params=model.parameters(), lr=learning_rate, betas=(decay1, decay2),\n",
    "                           weight_decay=weight_decay),\n",
    "        'SGD': optim.SGD(params=model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    }\n",
    "    # Choose which Torch Optimizer object to use, based on the passed name\n",
    "    optimizer = supported_optimizers[optimizer]\n",
    "\n",
    "    # stiamo dando ad \"args.load\" due compiti, quello di dirci il path e quello di dirci se caricare vecchi checkpoint\n",
    "    if load is not None:\n",
    "        model.load_state_dict(\n",
    "            torch.load(load, map_location=device)\n",
    "        )\n",
    "        logging.info(f'Model loaded from {load}')\n",
    "    model.to(device=device)\n",
    "\n",
    "    try:\n",
    "        train(model=model,\n",
    "              device=device,\n",
    "              train_loader=train_loader,\n",
    "              val_loader=val_loader,\n",
    "              max_epochs=max_epochs,\n",
    "              optimizer=optimizer,\n",
    "              verbose=verbose,\n",
    "              checkpoint_dir=checkpoint_dir,\n",
    "              model_path=model_path,\n",
    "              save_cp=save_cp,\n",
    "              n_train=n_train,\n",
    "              n_val=n_val\n",
    "              )\n",
    "    except KeyboardInterrupt:\n",
    "        torch.save(model.state_dict(), 'INTERRUPTED.ph')\n",
    "        logging.info('Interrupt saved')\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-18T12:02:20.678982Z",
     "start_time": "2020-12-18T12:02:11.829543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FlickrLogos-32 dataset...\n",
      "You have 9660 triplets\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss:   0%|          | 0/8694 [00:02<?, ?img/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 15.90 GiB total capacity; 7.24 GiB already allocated; 26.88 MiB free; 141.45 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-15fd78722232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m      \u001b[0mload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m      \u001b[0mval_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m      \u001b[0msave_cp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m      )\n",
      "\u001b[0;32m<ipython-input-3-e6f1a91b72f0>\u001b[0m in \u001b[0;36mtrain_main\u001b[0;34m(dataset, model, optimizer, max_epochs, batch_size, weight_decay, learning_rate, decay1, decay2, verbose, batch_norm, load, val_split, save_cp)\u001b[0m\n\u001b[1;32m    371\u001b[0m               \u001b[0msave_cp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_cp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m               \u001b[0mn_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m               \u001b[0mn_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m               )\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e6f1a91b72f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, val_loader, max_epochs, optimizer, verbose, checkpoint_dir, model_path, save_cp, n_train, n_val)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sending imgs to model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mpred_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mask correctly predicted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# print(pred_masks.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/alessandroaurora/one-shot_logo_detection/model/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, target)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# del x3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_conv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/alessandroaurora/one-shot_logo_detection/model/model_parts.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    785\u001b[0m         return F.conv_transpose2d(\n\u001b[1;32m    786\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             output_padding, self.groups, self.dilation)\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 15.90 GiB total capacity; 7.24 GiB already allocated; 26.88 MiB free; 141.45 MiB cached)"
     ]
    }
   ],
   "source": [
    "train_main(dataset=\"FlickrLogos-32\",\n",
    "     model=\"LogoDetection\",\n",
    "     optimizer=\"Adam\",\n",
    "     max_epochs=1, \n",
    "     batch_size=32, \n",
    "     weight_decay=5e-4,\n",
    "     learning_rate=4e-4,\n",
    "     decay1=0.9, \n",
    "     decay2=0.999,\n",
    "     verbose=True,\n",
    "     batch_norm=False,\n",
    "     load=None,\n",
    "     val_split=0.1,\n",
    "     save_cp=True,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T19:52:32.459982Z",
     "start_time": "2020-12-17T19:49:18.343Z"
    }
   },
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T19:52:32.461286Z",
     "start_time": "2020-12-17T19:49:20.111Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# todo: when we add more models, we should move these variables to another location\n",
    "ALL_MODEL_NAMES = [\"LogoDetection\"]\n",
    "ALL_DATASET_NAMES = [\"FlickrLogos-32, TopLogos-10\"]\n",
    "\n",
    "with open(os.path.abspath(\"./config/config.yaml\")) as config:\n",
    "    config_list = yaml.load(config, Loader=yaml.FullLoader)\n",
    "\n",
    "# def pred(model,\n",
    "#          sample,\n",
    "#          device,\n",
    "#          threshold=0.5):\n",
    "#     if(model.eval==False):\n",
    "#         model.eval()\n",
    "\n",
    "#     queries, targets = torch.from_numpy(BasicDataset.preprocess(index=index, file:files_path))\n",
    "#     queries = queries.unsqueeze(0)\n",
    "#     queries = queries.to(device=device, dtype=torch.float32)\n",
    "#     targets = targets.unsqueeze(0)\n",
    "#     targets = targets.to(device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def test(model,\n",
    "         device,\n",
    "         dataset,\n",
    "         batch_size,\n",
    "        #   save_path,\n",
    "         verbose: bool,\n",
    "         threshold=0.5):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # #TODO dataset preprocessing\n",
    "    # with torch.no_grad():\n",
    "    #     output = model(queries, targets)\n",
    "\n",
    "    #     probs = output.squeeze(0)\n",
    "    \n",
    "    logging.info(\"\\nPredicting image{} ... \")\n",
    "\n",
    "    with tqdm(total=len(dataset), desc=f'Testing dataset', unit='test-img', disable=not verbose) as bar:\n",
    "            bar.set_description(f'model testing')\n",
    "\n",
    "            for batch in test_loader:\n",
    "                queries = batch['query']  # Correct dimensions?\n",
    "                targets = batch['target']\n",
    "                bboxes = batch['bbox']\n",
    "\n",
    "                queries = queries.to(device=device, dtype=torch.float32)\n",
    "                targets = targets.to(device=device, dtype=torch.float32)\n",
    "                bboxes = bboxes.to(device=device, dtype=torch.float32)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred_masks = model(queries, targets)\n",
    "                    # print(pred_masks.shape)\n",
    "\n",
    "                bar.update(queries.shape[0])\n",
    "                global_step += 1\n",
    "                val_score += eval_net(model, batch, device)\n",
    "                # writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "\n",
    "                writer.add_images('query_images', queries, global_step)\n",
    "                writer.add_images('target_images', targets, global_step)\n",
    "                # writer.add_images('bboxes/true', bboxes, global_step)\n",
    "                writer.add_images('masks/pred', pred_masks, global_step)\n",
    "                writer.close()\n",
    "    return val_score / global_step\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset',\n",
    "                        choices=ALL_DATASET_NAMES,\n",
    "                        help=\"Dataset in {}\".format(ALL_DATASET_NAMES),\n",
    "                        required=True\n",
    "                        )\n",
    "\n",
    "    parser.add_argument('--model',\n",
    "                        choices=ALL_MODEL_NAMES,\n",
    "                        help=\"Model in {}\".format(ALL_MODEL_NAMES)\n",
    "                        )\n",
    "\n",
    "    parser.add_argument('--batch_size',\n",
    "                        default=32,\n",
    "                        type=int,\n",
    "                        help=\"Number of samples in each mini-batch in SGD and Adam optimization\"\n",
    "                        )\n",
    "\n",
    "    parser.add_argument('--verbose',\n",
    "                        default=True,\n",
    "                        type=bool,\n",
    "                        help=\"Verbose\"\n",
    "                        )\n",
    "\n",
    "    parser.add_argument('--load',\n",
    "                        type=str,\n",
    "                        required=True,\n",
    "                        help=\"Path to the model to load\"\n",
    "                        )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "    args = get_args()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    logging.info(f'Using device {device}')\n",
    "\n",
    "    # Modularized paths with respect to the current Dataset\n",
    "    imgs_dir = config_list['datasets'][args.dataset]['images']\n",
    "    masks_dir = config_list['datasets'][args.dataset]['masks']\n",
    "    # TODO: Controlla che ste due liste hanno le stesse sottocartelle\n",
    "    imgs_classes = [f.name for f in os.scandir(imgs_dir) if f.is_dir()]\n",
    "    mask_classes = [f.name for f in os.scandir(masks_dir) if f.is_dir()]\n",
    "    # checkpoint_dir = config_list['models'][args.model]['train_cp']\n",
    "\n",
    "    model_path = config_list['models'][args.model]['paths']['model']+ \"_\".join([args.model, args.dataset]) + \".pt\"\n",
    "\n",
    "    # print(\"Loading %s dataset...\" % args.dataset)\n",
    "    # dataset = BasicDataset(imgs_dir=imgs_dir, masks_dir=masks_dir)\n",
    "\n",
    "    # Change here to adapt your data\n",
    "    print(\"Initializing model...\")\n",
    "    model = LogoDetection(batch_norm=args.batch_norm,\n",
    "                        vgg_cfg=args.vgg_cfg)\n",
    "\n",
    "    model.load_state_dict(\n",
    "        torch.load(model_path, map_location=device)\n",
    "    )\n",
    "    logging.info(f'Model loaded from {model_path}')\n",
    "    model.to(device=device)\n",
    "\n",
    "    # Neo, enter in Metrics\n",
    "    metrics = []\n",
    "\n",
    "    for img_class_idx, img_class_path in enumerate(imgs_classes):\n",
    "        dataset = BasicDataset(imgs_dir=f\"{imgs_dir}{os.path.sep}{img_class_path}\", masks_dir=f\"{masks_dir}{os.path.sep}{masks_dir[img_class_idx]}\", dataset_name=args.dataset, skip_bbox_lines=1)\n",
    "        test_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "        try:\n",
    "            metrics.append(test(model=model,\n",
    "                                device=device,\n",
    "                                dataset=test_loader,\n",
    "                                batch_size=args.batch_size,\n",
    "                                verbose=args.verbose\n",
    "                                ))\n",
    "        except KeyboardInterrupt:\n",
    "            # torch.save(model.state_dict(), 'INTERRUPTED.ph')\n",
    "            # logging.info('Interrupt saved')\n",
    "            logging.info(\"Test interrupted\")\n",
    "            try:\n",
    "                sys.exit(0)\n",
    "            except SystemExit:\n",
    "                os._exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
